<p class="text-gray-700 mb-6">Great prompts are iterative. After launch, you’ll measure quality, speed, cost, and safety—then refine. This lesson gives you a practical framework to monitor real-world performance and continuously improve without breaking what works.</p>

<h3 class="text-2xl font-bold mb-4 border-b pb-2">What to Measure</h3>
<div class="overflow-x-auto mb-6">
  <table class="min-w-full text-sm border border-gray-200">
    <thead>
      <tr class="bg-gray-50">
        <th class="text-left px-4 py-2 border-b">Dimension</th>
        <th class="text-left px-4 py-2 border-b">Metrics</th>
        <th class="text-left px-4 py-2 border-b">Example Targets</th>
      </tr>
    </thead>
    <tbody class="text-gray-700">
      <tr>
        <td class="px-4 py-3 border-b">Quality</td>
        <td class="px-4 py-3 border-b">Task success rate, correctness, adherence to format</td>
        <td class="px-4 py-3 border-b">≥ 95% valid JSON; ≥ 85% human-rated correctness</td>
      </tr>
      <tr class="bg-white">
        <td class="px-4 py-3 border-b">Speed</td>
        <td class="px-4 py-3 border-b">Latency (p50/p95), timeouts</td>
        <td class="px-4 py-3 border-b">p95 &lt; 6s</td>
      </tr>
      <tr>
        <td class="px-4 py-3 border-b">Cost</td>
        <td class="px-4 py-3 border-b">Tokens/request, spend/day</td>
        <td class="px-4 py-3 border-b">≤ $X per 1K requests</td>
      </tr>
      <tr class="bg-white">
        <td class="px-4 py-3 border-b">Safety</td>
        <td class="px-4 py-3 border-b">Violation rate, redactions, refusals</td>
        <td class="px-4 py-3 border-b">Violations &lt; 0.1%</td>
      </tr>
      <tr>
        <td class="px-4 py-3 border-b">User Feedback</td>
        <td class="px-4 py-3 border-b">Thumbs up/down, reasons, comments</td>
        <td class="px-4 py-3 border-b">≥ 4.5/5 avg rating</td>
      </tr>
    </tbody>
  </table>
</div>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Logging Schema (Minimal but Useful)</h3>
<div class="bg-gray-50 p-6 rounded-lg mb-6 text-sm">
  <p class="font-semibold text-gray-800 mb-2">Capture just enough to diagnose issues while protecting privacy:</p>
  <pre class="bg-white p-3 rounded border border-gray-200 overflow-x-auto text-xs">{
  "ts": "2025-11-13T10:15:03Z",
  "sessionId": "anon-123",
  "promptVersion": "v7",
  "model": "gpt-4o-mini",
  "task": "summarize_ticket",
  "inputShape": {"chars": 1200, "hasPII": false},
  "latencyMs": 1820,
  "tokens": {"in": 850, "out": 220},
  "costUsd": 0.0042,
  "validSchema": true,
  "safety": {"violations": 0, "redactions": 1},
  "userFeedback": {"rating": 5, "reason": "clear"},
  "traceId": "abc123"
}</pre>
</div>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Offline Evaluations (Golden Set)</h3>
<ul class="list-disc pl-5 space-y-2 text-gray-700 text-sm mb-6">
  <li>Build a “golden dataset” of 50–200 real tasks with expected outputs</li>
  <li>Score automatically: schema validity, keyword/regex checks, simple heuristics</li>
  <li>Use human review for nuanced criteria (tone, completeness)</li>
  <li>Run before/after every change to catch regressions</li>
</ul>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Online Tests: A/B, Shadow, Canary</h3>
<div class="overflow-x-auto mb-6 text-sm">
  <table class="min-w-full border border-gray-200">
    <thead>
      <tr class="bg-blue-50">
        <th class="text-left px-4 py-2 border-b">Method</th>
        <th class="text-left px-4 py-2 border-b">Use When</th>
        <th class="text-left px-4 py-2 border-b">Notes</th>
      </tr>
    </thead>
    <tbody class="text-gray-700">
      <tr>
        <td class="px-4 py-3 border-b">A/B Test</td>
        <td class="px-4 py-3 border-b">Compare two prompts/params</td>
        <td class="px-4 py-3 border-b">Split traffic; watch success, cost, latency</td>
      </tr>
      <tr class="bg-white">
        <td class="px-4 py-3 border-b">Shadow</td>
        <td class="px-4 py-3 border-b">Evaluate new version without affecting users</td>
        <td class="px-4 py-3 border-b">Duplicate requests; log results silently</td>
      </tr>
      <tr>
        <td class="px-4 py-3 border-b">Canary</td>
        <td class="px-4 py-3 border-b">Gradual rollout to 1% → 10% → 50%</td>
        <td class="px-4 py-3 border-b">Rollback on regression thresholds</td>
      </tr>
    </tbody>
  </table>
</div>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Prompt Versioning & Change Control</h3>
<ul class="list-disc pl-5 space-y-2 text-gray-700 text-sm mb-6">
  <li>Store prompts in versioned files; include change log and owner</li>
  <li>Track model version and parameters (temperature, top_p, max_tokens)</li>
  <li>Gate changes behind evaluations and A/B or canary rollouts</li>
  <li>Keep rollback plan: last good prompt+params ID</li>
</ul>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Error Taxonomy</h3>
<div class="bg-gray-50 p-6 rounded-lg mb-6 text-sm">
  <ul class="list-disc pl-5 space-y-1 text-gray-700">
    <li>Factual error (wrong info)</li>
    <li>Missing content (incomplete)</li>
    <li>Format error (invalid JSON/structure)</li>
    <li>Policy violation (safety/compliance)</li>
    <li>Style/tone mismatch</li>
    <li>Latency/timeout</li>
  </ul>
</div>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Continuous Improvement Loop</h3>
<ol class="list-decimal pl-5 space-y-2 text-gray-700 text-sm mb-6">
  <li>Collect logs + feedback</li>
  <li>Cluster failures by taxonomy</li>
  <li>Hypothesize fixes (prompt edits, new examples, different params)</li>
  <li>Run offline evals on golden set</li>
  <li>Ship via canary; monitor dashboards</li>
  <li>Promote or rollback; document learnings</li>
</ol>

<h3 class="text-2xl font-bold mb-4 mt-8 border-b pb-2">Mini Exercise: Build a Tiny Eval</h3>
<div class="bg-green-50 border-l-4 border-green-500 p-6 rounded-r mb-6">
  <p class="font-semibold text-green-900 mb-3">Exercise Task:</p>
  <p class="text-gray-700 mb-4">You have a summarization prompt that must output JSON with fields <code>title</code> and <code>bullets[]</code>. Create an eval plan.</p>
  <ol class="list-decimal pl-5 space-y-2 text-gray-700 text-sm mb-3">
    <li>Define 10 golden inputs (ticket texts)</li>
    <li>Write validators: JSON parseable, has title, 3–7 bullets, each under 120 chars</li>
    <li>Pick success threshold: ≥ 9/10 pass</li>
    <li>Propose one prompt tweak and how you’ll test it</li>
  </ol>
  <details class="mt-2">
    <summary class="cursor-pointer font-semibold text-green-900 hover:text-green-700">Click to see sample solution</summary>
    <div class="mt-3 p-4 bg-white rounded border border-green-200 text-sm">
      <p><span class="font-semibold">Validator snippet (pseudocode):</span></p>
      <pre class="bg-gray-50 p-3 rounded text-xs overflow-x-auto">function validate(out){
  try { var j = JSON.parse(out) } catch(e){ return false }
  if(!j.title || !Array.isArray(j.bullets)) return false
  if(j.bullets.length < 3 || j.bullets.length > 7) return false
  return j.bullets.every(b => b.length <= 120)
}</pre>
    </div>
  </details>
</div>

<div class="bg-yellow-50 border-l-4 border-yellow-400 p-4 text-yellow-800 rounded-r">
  <p class="font-semibold mb-2">Key Takeaway:</p>
  <p class="italic">You can’t improve what you don’t measure. Instrument your prompts, run small but meaningful evals, and ship changes behind guardrails. Over time, your system gets cheaper, faster, and more accurate.</p>
</div>
